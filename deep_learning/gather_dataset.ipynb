{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from joblib import delayed, Parallel\n",
    "from towbintools.foundation.image_handling import read_tiff_file\n",
    "from tifffile import imwrite\n",
    "\n",
    "# set random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "np.random.seed(387799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_cluster_path = \"/mnt/towbin.data/shared\"\n",
    "valid_experimentalists = [\"plenart\"]\n",
    "\n",
    "valid_scopes = [\"squid\"]\n",
    "body_strains = []\n",
    "pharynx_strains = []\n",
    "germline_strains = [\"530\"]\n",
    "\n",
    "keywords_to_exclude = [\"exclude\", \"fail\", \"failure\", \"crash\"]\n",
    "# experient_to_always_include = [\"20250314_squid_10x_yap_aid_160_438_492_493\"]\n",
    "experient_to_always_include = []\n",
    "experiments_to_exclude = [\n",
    "    \"20241111_squid_10x_wBT318_NaCl\",\n",
    "    \"20241707_souvik_w318_ev_mex3_squid\",\n",
    "    \"20240515_squid1_10x_wbt160_25C_2024-05-15_15-31-45.506118\",\n",
    "    \"20240212_squid_wbt318_Nacl\",\n",
    "    \"20241912_squid_10x_wBT_344_415\",\n",
    "    \"20252401_squid_10x_wBT318_reproduction\",\n",
    "    \"20252001_squid_10x_wBT318_NaCl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"/mnt/towbin.data/shared/spsalmon/towbinlab_segmentation_database/crosstalk_germline\"\n",
    "\n",
    "db_organs = ['germline']\n",
    "\n",
    "extra_adulthood_time = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(database_path, exist_ok=True)\n",
    "\n",
    "if \"body\" in db_organs:\n",
    "    body_database_path = os.path.join(database_path, \"body\")\n",
    "    os.makedirs(body_database_path, exist_ok=True)\n",
    "    body_images = os.path.join(body_database_path, \"images\")\n",
    "    os.makedirs(body_images, exist_ok=True)\n",
    "    body_masks = os.path.join(body_database_path, \"masks\")\n",
    "    os.makedirs(body_masks, exist_ok=True)\n",
    "    body_images_bf = os.path.join(body_database_path, \"images_bf\")\n",
    "    os.makedirs(body_images_bf, exist_ok=True)\n",
    "\n",
    "if \"pharynx\" in db_organs:\n",
    "    pharynx_database_path = os.path.join(database_path, \"pharynx\")\n",
    "    os.makedirs(pharynx_database_path, exist_ok=True)\n",
    "    pharynx_images = os.path.join(pharynx_database_path, \"images\")\n",
    "    os.makedirs(pharynx_images, exist_ok=True)\n",
    "    pharynx_masks = os.path.join(pharynx_database_path, \"masks\")\n",
    "    os.makedirs(pharynx_masks, exist_ok=True)\n",
    "\n",
    "if \"germline\" in db_organs:\n",
    "    germline_database_path = os.path.join(database_path, \"germline\")\n",
    "    os.makedirs(germline_database_path, exist_ok=True)\n",
    "    germline_images = os.path.join(germline_database_path, \"images\")\n",
    "    os.makedirs(germline_images, exist_ok=True)\n",
    "    germline_masks = os.path.join(germline_database_path, \"masks\")\n",
    "    os.makedirs(germline_masks, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configurations\n",
    "# database_configs = {\n",
    "#     'body': {\n",
    "#         'size': 3000,\n",
    "#         'stage_proportions': {'egg': 0.1, 'adult': 0.1, 'L1': 0.2, 'L2': 0.2, 'L3': 0.2, 'L4': 0.2},\n",
    "#         'scope_proportions': {'crest': 0.2, 'squid': 0.2, 'ti2': 0.6},\n",
    "#         'strains': body_strains,\n",
    "#         'output_path': body_database_path\n",
    "#     },\n",
    "#     'pharynx': {\n",
    "#         'size': 3000,\n",
    "#         'stage_proportions': {'adult': 0.1, 'L1': 0.225, 'L2': 0.225, 'L3': 0.225, 'L4': 0.225},\n",
    "#         'scope_proportions': {'ti2': 1.0},\n",
    "#         'strains': pharynx_strains,\n",
    "#         'output_path': pharynx_database_path\n",
    "#     },\n",
    "#     'germline': {\n",
    "#         'size': 3000,\n",
    "#         'stage_proportions': {'adult': 0.2, 'L1': 0.2, 'L2': 0.2, 'L3': 0.2, 'L4': 0.2},\n",
    "#         'scope_proportions': {'crest': 0.6, 'squid': 0.4},\n",
    "#         'strains': germline_strains,\n",
    "#         'output_path': germline_database_path\n",
    "#     }\n",
    "# }\n",
    "\n",
    "database_configs = {\n",
    "    'germline': {\n",
    "        'size': 1000,\n",
    "        'stage_proportions': {'adult': 0.2, 'L1': 0.2, 'L2': 0.2, 'L3': 0.2, 'L4': 0.2},\n",
    "        'scope_proportions': {'squid': 1.0},\n",
    "        'strains': germline_strains,\n",
    "        'output_path': germline_database_path\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strains = body_strains + pharynx_strains + germline_strains\n",
    "# remove repeated strains\n",
    "all_strains = list(set(all_strains))\n",
    "strains_to_exclude = []\n",
    "\n",
    "# to be valid, the strain name needs to be followed by either a dash or an underscore\n",
    "correct_body_strains = []\n",
    "correct_pharynx_strains = []\n",
    "correct_germline_strains = []\n",
    "for strain in body_strains:\n",
    "    correct_body_strains.append(strain + \"-\")\n",
    "    correct_body_strains.append(strain + \"_\")\n",
    "    correct_body_strains.append(strain + \" \")\n",
    "\n",
    "for strain in pharynx_strains:\n",
    "    correct_pharynx_strains.append(strain + \"-\")\n",
    "    correct_pharynx_strains.append(strain + \"_\")\n",
    "    correct_pharynx_strains.append(strain + \" \")\n",
    "\n",
    "for strain in germline_strains:\n",
    "    correct_germline_strains.append(strain + \"-\")\n",
    "    correct_germline_strains.append(strain + \"_\")\n",
    "    correct_germline_strains.append(strain + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 valid experiments\n",
      "['/mnt/towbin.data/shared/plenart/20252305_squid_10x_wBT530_IAA_20_degrees/analysis_Peter/report/analysis_filemap_annotated.csv']\n"
     ]
    }
   ],
   "source": [
    "# 1. the experiment must have a filemap in the new python format somewhere in its directory\n",
    "# 2. the experiment must be from 2022-TODAY\n",
    "# 3. the experiment must have the name of the scope in its name\n",
    "# 4. the experiment must contain \"10x\" in its name\n",
    "# 5. the experiment must be from Klement, Ioana or Peter\n",
    "# 6. the experiment must include in its name at least one of the relevant strain names from a list\n",
    "# 7. the experiment must not have some keyword in its name (e.g. \"exclude\", \"fail\")\n",
    "\n",
    "# when taking data for different organs, different images must be used (as to not bias the network)\n",
    "# when taking in data for the body, also take the brightfield channel\n",
    "\n",
    "\n",
    "def get_analysis_filemap(experiment_path):\n",
    "    directories = [\n",
    "        os.path.join(experiment_path, d)\n",
    "        for d in os.listdir(experiment_path)\n",
    "        if os.path.isdir(os.path.join(experiment_path, d))\n",
    "    ]\n",
    "\n",
    "    analysis_directories = [d for d in directories if \"analysis\" in d]\n",
    "    report_directories = [os.path.join(d, \"report\") for d in analysis_directories]\n",
    "\n",
    "    report_directories = [d for d in report_directories if os.path.isdir(d)]\n",
    "\n",
    "    for report_dir in report_directories:\n",
    "        files = [os.path.join(report_dir, f) for f in os.listdir(report_dir)]\n",
    "        filemap_files = [f for f in files if \"analysis_filemap_annotated\" in f]\n",
    "        mat_files = [f for f in files if \".mat\" in f]\n",
    "\n",
    "        # return the filemap that was created last\n",
    "        if filemap_files:\n",
    "            filemap_files.sort(key=lambda x: os.path.getctime(x))\n",
    "            return os.path.join(report_dir, filemap_files[-1])\n",
    "        # check for converted experiments\n",
    "        elif mat_files:\n",
    "            filemap_files = [f for f in files if \"analysis_filemap\" in f]\n",
    "            if filemap_files:\n",
    "                filemap_files.sort(key=lambda x: os.path.getctime(x))\n",
    "                filemap = pd.read_csv(filemap_files[-1], low_memory=False)\n",
    "                if \"HatchTime\" in filemap.columns and \"raw\" in filemap.columns:\n",
    "                    return os.path.join(report_dir, filemap_files[-1])\n",
    "    return None\n",
    "\n",
    "# create a lot of possible variation of the scope names\n",
    "valid_scopes_variations = []\n",
    "for scope in valid_scopes:\n",
    "    valid_scopes_variations.append(scope)\n",
    "    valid_scopes_variations.append(scope.upper())\n",
    "    valid_scopes_variations.append(scope.capitalize())\n",
    "\n",
    "filemaps = []\n",
    "\n",
    "valid_experimentalists_dir = [\n",
    "    os.path.join(storage_cluster_path, exp) for exp in valid_experimentalists\n",
    "]\n",
    "\n",
    "for exp_dir in valid_experimentalists_dir:\n",
    "    experiment_directories = [\n",
    "        os.path.join(exp_dir, d)\n",
    "        for d in os.listdir(exp_dir)\n",
    "        if os.path.isdir(os.path.join(exp_dir, d))\n",
    "    ]\n",
    "\n",
    "    for exp in experiment_directories:\n",
    "        experiment_name = os.path.basename(os.path.normpath(exp))\n",
    "\n",
    "        # check if the experiment is in the list of experiments to always include\n",
    "        if experiment_name in experient_to_always_include:\n",
    "            filemap = get_analysis_filemap(exp)\n",
    "            if filemap:\n",
    "                filemaps.append(filemap)\n",
    "                continue\n",
    "        # check if the experiment is in the list of experiments to exclude\n",
    "        if experiment_name in experiments_to_exclude:\n",
    "            continue\n",
    "        try:\n",
    "            year = int(experiment_name[:4])\n",
    "            if year < 2023:\n",
    "                continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if \"10x\" not in experiment_name and \"10X\" not in experiment_name:\n",
    "            continue\n",
    "\n",
    "        if not any(scope in experiment_name for scope in valid_scopes_variations):\n",
    "            continue\n",
    "\n",
    "        if any(keyword in experiment_name for keyword in keywords_to_exclude):\n",
    "            continue\n",
    "\n",
    "        if not any(strain in experiment_name for strain in all_strains):\n",
    "            continue\n",
    "\n",
    "        if any(strain in experiment_name for strain in strains_to_exclude):\n",
    "            continue\n",
    "\n",
    "        filemap = get_analysis_filemap(exp)\n",
    "\n",
    "        if filemap:\n",
    "            filemaps.append(filemap)\n",
    "\n",
    "print(f\"Found {len(filemaps)} valid experiments\")\n",
    "\n",
    "print(filemaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/towbin.data/shared/plenart/20252305_squid_10x_wBT530_IAA_20_degrees/analysis_Peter/report/analysis_filemap_annotated.csv\n",
      "Error in picking image within larval stage: 0, 1.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 6.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 0.0\n",
      "Error in picking image within larval stage: 0, 5.0\n",
      "Error in picking image within larval stage: 0, 5.0\n",
      "Error in picking image within larval stage: 0, 2.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 5.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 8.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 3.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 3.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 6.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 3.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 8.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 8.0\n",
      "Error in picking image within larval stage: 0, 2.0\n",
      "Error in picking image within larval stage: 0, 3.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 5.0\n",
      "Error in picking image within larval stage: 0, 3.0\n",
      "Error in picking image within larval stage: 0, 1.0\n",
      "Error in picking image within larval stage: 0, 6.0\n",
      "Error in picking image within larval stage: 0, 4.0\n",
      "Error in picking image within larval stage: 0, 5.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 6.0\n",
      "Error in picking image within larval stage: 0, 1.0\n",
      "Error in picking image within larval stage: 0, 5.0\n",
      "Error in picking image within larval stage: 0, 7.0\n",
      "Error in picking image within larval stage: 0, 2.0\n"
     ]
    }
   ],
   "source": [
    "# get the images from the filemaps\n",
    "def pick_within_larval_stage(images, times, ls_beg, ls_end, n_picks=1):\n",
    "    try:\n",
    "        if np.isnan(ls_beg) or np.isnan(ls_end):\n",
    "            return None, None\n",
    "        \n",
    "        valid_stacks = [(s, t) for s, t in zip(images, times) if ls_beg <= t <= ls_end]\n",
    "        \n",
    "        if valid_stacks:\n",
    "            # Handle case where npicks is larger than available stacks\n",
    "            npicks = min(n_picks, len(valid_stacks))\n",
    "            \n",
    "            # Randomly select npicks number of stacks without replacement\n",
    "            selected_indices = np.random.choice(len(valid_stacks), size=n_picks, replace=False)\n",
    "            selected_stacks = [valid_stacks[i] for i in selected_indices]\n",
    "            \n",
    "            # Unzip the selected stacks into separate lists\n",
    "            selected_images, selected_times = zip(*selected_stacks)\n",
    "            \n",
    "            # If npicks is 1, return single values to maintain backward compatibility\n",
    "            if npicks == 1:\n",
    "                return selected_images[0], selected_times[0]\n",
    "            \n",
    "            return list(selected_images), list(selected_times)\n",
    "        else:\n",
    "            if n_picks == 1:\n",
    "                return None, None\n",
    "            return [], []\n",
    "    except:\n",
    "        print(f'Error in picking image within larval stage: {ls_beg}, {ls_end}')\n",
    "        if npicks == 1:\n",
    "            return None, None\n",
    "        return [], []\n",
    "        \n",
    "database_filemap = pd.DataFrame()\n",
    "\n",
    "for filemap in filemaps:\n",
    "    print(filemap)\n",
    "    experiment_name = filemap.split(\"/\")[-4]\n",
    "    filemap_df = pd.read_csv(filemap)\n",
    "\n",
    "    strain = [strain for strain in all_strains if strain in experiment_name][0]\n",
    "\n",
    "    # volume_column = [col for col in filemap_df.columns if \"volume\" in col][0]\n",
    "    # print(volume_column)\n",
    "\n",
    "    worm_type_column = [col for col in filemap_df.columns if \"worm_type\" in col][0]\n",
    "    # print(worm_type_column)\n",
    "    \n",
    "    # get the name of the microscope by matching the filemap path with the valid scopes\n",
    "    microscope = [scope for scope in valid_scopes_variations if scope in filemap][0]\n",
    "\n",
    "    strain\n",
    "\n",
    "    if microscope in ['crest', 'Crest', 'CREST', 'squid']:\n",
    "        n_picks = 10\n",
    "    else:\n",
    "        n_picks = 2\n",
    "\n",
    "    for point in filemap_df['Point'].unique():\n",
    "        rows = []\n",
    "        point_df = filemap_df[filemap_df['Point'] == point]\n",
    "        hatch_time, m1, m2, m3, m4 = point_df['HatchTime'].values[0], point_df['M1'].values[0], point_df['M2'].values[0], point_df['M3'].values[0], point_df['M4'].values[0]\n",
    "        raw_images = point_df['raw'].values\n",
    "        time = point_df['Time'].values\n",
    "        worm_type = point_df[worm_type_column].values\n",
    "        \n",
    "        # ignore this point if there are more than 50% non worm/egg images\n",
    "        if len([wt for wt in worm_type if wt in ['worm', 'egg']]) < 0.5 * len(worm_type):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            stacks = [os.path.join(os.path.dirname(filemap), img) for img in raw_images]\n",
    "            stacks_time = time\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Handle egg stage\n",
    "        egg_images, egg_image_times = pick_within_larval_stage(stacks, stacks_time, 0, hatch_time, n_picks=n_picks)\n",
    "        if egg_images:  # Will be a list when n_picks > 1\n",
    "            if not isinstance(egg_images, list):  # Handle single pick case\n",
    "                egg_images = [egg_images]\n",
    "            for egg_image in egg_images:\n",
    "                egg_image = egg_image.replace(\"external.data/TowbinLab\", \"towbin.data/shared\")\n",
    "                row = {'Point': point, 'Image': egg_image, 'Stage': 0, 'Microscope': microscope, 'Experiment': experiment_name, 'Strain': strain}\n",
    "                rows.append(row)\n",
    "\n",
    "        # Handle larval stages\n",
    "        for i, (ls_beg, ls_end) in enumerate([(hatch_time, m1), (m1, m2), (m2, m3), (m3, m4)]):\n",
    "            stage_images, stage_times = pick_within_larval_stage(stacks, stacks_time, ls_beg, ls_end, n_picks=n_picks)\n",
    "            if stage_images:  # Will be a list when n_picks > 1\n",
    "                if not isinstance(stage_images, list):  # Handle single pick case\n",
    "                    stage_images = [stage_images]\n",
    "                for stage_image in stage_images:\n",
    "                    stage_image = stage_image.replace(\"external.data/TowbinLab\", \"towbin.data/shared\")\n",
    "                    row = {'Point': point, 'Image': stage_image, 'Stage': i+1, 'Microscope': microscope, 'Experiment': experiment_name, 'Strain': strain}\n",
    "                    rows.append(row)\n",
    "\n",
    "        # Handle adult stage\n",
    "        try:\n",
    "            adult_images, adult_image_times = pick_within_larval_stage(\n",
    "                stacks, \n",
    "                stacks_time, \n",
    "                m4, \n",
    "                np.min([m4 + extra_adulthood_time, np.max(stacks_time)]),\n",
    "                n_picks=n_picks\n",
    "            )\n",
    "        except:\n",
    "            adult_images = None\n",
    "\n",
    "        if adult_images:  # Will be a list when n_picks > 1\n",
    "            if not isinstance(adult_images, list):  # Handle single pick case\n",
    "                adult_images = [adult_images]\n",
    "            for adult_image in adult_images:\n",
    "                adult_image = adult_image.replace(\"external.data/TowbinLab\", \"towbin.data/shared\")\n",
    "                row = {'Point': point, 'Image': adult_image, 'Stage': 5, 'Microscope': microscope, 'Experiment': experiment_name, 'Strain': strain}\n",
    "                rows.append(row)\n",
    "\n",
    "        database_filemap = pd.concat([database_filemap, pd.DataFrame(rows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "database_filemap.reset_index(drop=True, inplace=True)\n",
    "database_filemap.to_csv(os.path.join(database_path, \"database_filemap.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating germline database...\n",
      "Created germline database with 1000 images\n"
     ]
    }
   ],
   "source": [
    "# Define microscope name mappings\n",
    "microscope_names = {'crest': ['Crest', 'crest', 'CREST'], \n",
    "                    'squid': ['Squid', 'squid', 'SQUID'], \n",
    "                    'ti2': ['Ti2', 'ti2', 'TI2', 'orca', 'Orca', 'ORCA']}\n",
    "\n",
    "# Create mapping from variations to standard names\n",
    "variation_to_unified_name = {}\n",
    "for microscope, variations in microscope_names.items():\n",
    "    for variation in variations:\n",
    "        variation_to_unified_name[variation] = microscope\n",
    "\n",
    "# Add standardized microscope names to database\n",
    "database_filemap['CorrectMicroscopeName'] = database_filemap['Microscope'].apply(\n",
    "    lambda x: variation_to_unified_name[x]\n",
    ")\n",
    "\n",
    "def calculate_image_combinations(\n",
    "    database_size,\n",
    "    scope_proportions,\n",
    "    stage_proportions\n",
    "):\n",
    "    \"\"\"Calculate number of images needed for each scope and stage combination.\"\"\"\n",
    "    stage_to_number = {'egg': 0, 'L1': 1, 'L2': 2, 'L3': 3, 'L4': 4, 'adult': 5}\n",
    "    \n",
    "    combinations = {}\n",
    "    \n",
    "    # Calculate for each scope and stage combination\n",
    "    for scope, scope_prop in scope_proportions.items():\n",
    "        combinations[scope] = {}\n",
    "        for stage, stage_prop in stage_proportions.items():\n",
    "            n_images = int(database_size * scope_prop * stage_prop)\n",
    "            stage_number = stage_to_number[stage]\n",
    "            combinations[scope][stage_number] = n_images\n",
    "    \n",
    "    return combinations\n",
    "\n",
    "def create_database(\n",
    "    database_name,\n",
    "    database_size,\n",
    "    stage_proportions,\n",
    "    scope_proportions,\n",
    "    strains,\n",
    "    output_path,\n",
    "    available_data\n",
    "):\n",
    "    \"\"\"Create a database with specified proportions and parameters.\n",
    "    \n",
    "    Args:\n",
    "        database_name: Name of the database (for logging)\n",
    "        database_size: Total number of images to include\n",
    "        stage_proportions: Dictionary mapping stages to their proportion\n",
    "        scope_proportions: Dictionary mapping microscopes to their proportion\n",
    "        strains: List of strains to include\n",
    "        output_path: Path to save the resulting CSV\n",
    "        available_data: DataFrame containing available images to sample from\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (created database, remaining available data)\n",
    "    \"\"\"\n",
    "    print(f\"Creating {database_name} database...\")\n",
    "    \n",
    "    # Create empty database\n",
    "    database = pd.DataFrame()\n",
    "    \n",
    "    # Calculate combinations\n",
    "    combinations = calculate_image_combinations(\n",
    "        database_size, \n",
    "        stage_proportions=stage_proportions, \n",
    "        scope_proportions=scope_proportions\n",
    "    )\n",
    "    \n",
    "    # For each scope and stage, sample the required number of images\n",
    "    for scope, stages in combinations.items():\n",
    "        for stage, n_images in stages.items():\n",
    "            try:\n",
    "                stage_images = available_data[\n",
    "                    (available_data['CorrectMicroscopeName'] == scope) & \n",
    "                    (available_data['Stage'] == stage) &\n",
    "                    (available_data['Strain'].isin(strains))\n",
    "                ].sample(n=n_images)\n",
    "                \n",
    "                database = pd.concat([database, stage_images])\n",
    "            except:\n",
    "                print(f\"Warning: Could not sample {n_images} images for {scope}, stage {stage}\")\n",
    "                continue\n",
    "    \n",
    "    # Reset index and save\n",
    "    database.reset_index(drop=True, inplace=True)\n",
    "    database.to_csv(os.path.join(output_path, f\"{database_name}_database_filemap.csv\"), index=False)\n",
    "    \n",
    "    # Remove selected images from available data\n",
    "    remaining_data = available_data[~available_data['Image'].isin(database['Image'])]\n",
    "    \n",
    "    return database, remaining_data\n",
    "\n",
    "# Create all databases sequentially\n",
    "available_data = database_filemap.copy()\n",
    "databases = {}\n",
    "\n",
    "for db_name, config in database_configs.items():\n",
    "    databases[db_name], available_data = create_database(\n",
    "        database_name=db_name,\n",
    "        database_size=config['size'],\n",
    "        stage_proportions=config['stage_proportions'],\n",
    "        scope_proportions=config['scope_proportions'],\n",
    "        strains=config['strains'],\n",
    "        output_path=config['output_path'],\n",
    "        available_data=available_data\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {db_name} database with {len(databases[db_name])} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, output_dir, channel):\n",
    "    image_path = row['Image']\n",
    "    image = read_tiff_file(image_path)\n",
    "    channel = channel[0] if isinstance(channel, list) else channel\n",
    "    if channel >= image.shape[0]:\n",
    "        channel = image.shape[0] - 1\n",
    "    image = image[channel, :, :]  # select the channel\n",
    "    image_name = row['OutputName']\n",
    "    # save the image\n",
    "    imwrite(os.path.join(output_dir, image_name), image, compression=\"zlib\")\n",
    "\n",
    "def get_masks(row, output_dir, channel):\n",
    "    image_path = row['Image']\n",
    "    mask_path = image_path.replace(\"raw\", f\"analysis/ch{channel[0]+1}_seg\")\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Mask not found for {image_path}, skipping.\")\n",
    "        return\n",
    "    mask = read_tiff_file(mask_path)\n",
    "    mask_name = row['OutputName']\n",
    "    imwrite(os.path.join(output_dir, mask_name), mask, compression=\"zlib\")\n",
    "\n",
    "# extract the images from the database\n",
    "def extract_images(database, output_dir, channel):\n",
    "    Parallel(n_jobs=32, prefer=\"threads\")(delayed(process_row)(row, output_dir, channel) for _, row in database.iterrows())\n",
    "\n",
    "def extract_masks(database, output_dir, channel):\n",
    "    Parallel(n_jobs=32, prefer=\"threads\")(delayed(get_masks)(row, output_dir, channel) for _, row in database.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_name(row):\n",
    "    return f'image_{row.name}_{row[\"CorrectMicroscopeName\"]}_{row[\"Stage\"]}.tif'\n",
    "\n",
    "if \"body\" in db_organs:\n",
    "    body_database = databases['body']\n",
    "    body_database['OutputName'] = body_database.apply(get_output_name, axis=1)\n",
    "    body_database.to_csv(os.path.join(body_database_path, \"body_database_filemap.csv\"), index=False)\n",
    "    body_database = body_database.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    extract_images(body_database, body_images, [1])\n",
    "\n",
    "if \"germline\" in db_organs:\n",
    "    germline_database = databases['germline']\n",
    "    germline_database['OutputName'] = germline_database.apply(get_output_name, axis=1)\n",
    "    germline_database.to_csv(os.path.join(germline_database_path, \"germline_database_filemap.csv\"), index=False)\n",
    "    germline_database = germline_database.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    extract_images(germline_database, germline_images, [0])\n",
    "\n",
    "if \"pharynx\" in db_organs:\n",
    "    pharynx_database = databases['pharynx']\n",
    "    pharynx_database['OutputName'] = pharynx_database.apply(get_output_name, axis=1)\n",
    "    pharynx_database.to_csv(os.path.join(pharynx_database_path, \"pharynx_database_filemap.csv\"), index=False)\n",
    "    pharynx_database = pharynx_database.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    extract_images(pharynx_database, pharynx_images, [0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
