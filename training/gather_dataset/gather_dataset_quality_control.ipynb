{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9bbb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microscope found: ti2 in experiment 20232707_Ti2_wbt125_measuring_measurement_error_for_organ_size_paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:43<00:00, 19.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import yaml\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "from tifffile import imwrite\n",
    "from towbintools.foundation.image_handling import read_tiff_file\n",
    "from towbintools.data_analysis.time_series import smooth_series_classified, correct_series_with_classification\n",
    "from towbintools.foundation.image_quality import TENG_VARIANCE\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "np.random.seed(387799)\n",
    "\n",
    "def process_strains(strains):\n",
    "    # to be correct the strain number needs to be followed by a separator like -, _ or space\n",
    "    correct_strains = []\n",
    "    if strains is None:\n",
    "        return correct_strains\n",
    "    for strain in strains:\n",
    "        correct_strains.append(strain + \"-\")\n",
    "        correct_strains.append(strain + \"_\")\n",
    "        correct_strains.append(strain + \" \")\n",
    "    return correct_strains\n",
    "\n",
    "\n",
    "def get_analysis_filemap(experiment_path, get_annotated_only=False):\n",
    "    directories = [\n",
    "        os.path.join(experiment_path, d)\n",
    "        for d in os.listdir(experiment_path)\n",
    "        if os.path.isdir(os.path.join(experiment_path, d))\n",
    "    ]\n",
    "\n",
    "    analysis_directories = [d for d in directories if \"analysis\" in d]\n",
    "    report_directories = [os.path.join(d, \"report\") for d in analysis_directories]\n",
    "\n",
    "    report_directories = [d for d in report_directories if os.path.isdir(d)]\n",
    "\n",
    "    filemap_files = []\n",
    "    for report_dir in report_directories:\n",
    "        files = [os.path.join(report_dir, f) for f in os.listdir(report_dir)]\n",
    "        if get_annotated_only:\n",
    "            files = [f for f in files if \"analysis_filemap_annotated\" in f]\n",
    "        else:\n",
    "            files = [f for f in files if \"analysis_filemap\" in f]\n",
    "        if files:\n",
    "            filemap_files.sort(key=lambda x: os.path.getctime(x))\n",
    "            filemap_files.append(os.path.join(report_dir, files[-1]))\n",
    "\n",
    "    if filemap_files:\n",
    "        filemap_files.sort(key=lambda x: os.path.getctime(x))\n",
    "        return os.path.join(report_dir, filemap_files[-1])\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_all_relevant_filemaps(\n",
    "    experiment_directories,\n",
    "    experiments_to_always_include,\n",
    "    keywords_to_include,\n",
    "    experiments_to_exclude,\n",
    "    valid_scopes_expanded,\n",
    "    keywords_to_exclude,\n",
    "    database_config,\n",
    "    get_annotated_only=False,\n",
    "):\n",
    "    filemaps = []\n",
    "    strains = process_strains(database_config.get(\"strains\", []))\n",
    "    magnifications = database_config.get(\"magnifications\", [])\n",
    "    processed_magnifications = []\n",
    "    for mag in magnifications:\n",
    "        processed_magnifications.append(mag)\n",
    "        processed_magnifications.append(mag.lower())\n",
    "        processed_magnifications.append(mag.upper())\n",
    "\n",
    "    magnifications = list(set(processed_magnifications))\n",
    "\n",
    "    for exp in experiment_directories:\n",
    "        experiment_name = os.path.basename(os.path.normpath(exp))\n",
    "\n",
    "        # check if the experiment is in the list of experiments to always include\n",
    "        if experiment_name in experiments_to_always_include:\n",
    "            filemap = get_analysis_filemap(exp, get_annotated_only=get_annotated_only)\n",
    "            if filemap:\n",
    "                filemaps.append(filemap)\n",
    "                continue\n",
    "\n",
    "        if any(keyword in experiment_name for keyword in keywords_to_include):\n",
    "            filemap = get_analysis_filemap(exp, get_annotated_only=get_annotated_only)\n",
    "            if filemap:\n",
    "                filemaps.append(filemap)\n",
    "                continue\n",
    "\n",
    "        # check if the experiment is in the list of experiments to exclude\n",
    "        if experiment_name in experiments_to_exclude:\n",
    "            continue\n",
    "\n",
    "        if not any(mag in experiment_name for mag in magnifications) and magnifications:\n",
    "            continue\n",
    "\n",
    "        if not any(scope in experiment_name for scope in valid_scopes_expanded):\n",
    "            continue\n",
    "\n",
    "        if any(keyword in experiment_name for keyword in keywords_to_exclude):\n",
    "            continue\n",
    "\n",
    "        if not any(strain in experiment_name for strain in strains) and strains:\n",
    "            continue\n",
    "\n",
    "        filemap = get_analysis_filemap(exp, get_annotated_only=get_annotated_only)\n",
    "\n",
    "        if filemap:\n",
    "            filemaps.append(filemap)\n",
    "\n",
    "    return filemaps\n",
    "\n",
    "\n",
    "def pick_within_larval_stage(filemap, ls_beg, ls_end, n_picks=1):\n",
    "    try:\n",
    "        if np.isnan(ls_beg) or np.isnan(ls_end):\n",
    "            return []\n",
    "\n",
    "        filemap_of_stage = filemap.filter(\n",
    "            (pl.col(\"Time\") >= ls_beg) & (pl.col(\"Time\") <= ls_end)\n",
    "        )\n",
    "\n",
    "        if filemap_of_stage.height > 0:\n",
    "            picks = min(n_picks, filemap_of_stage.height)\n",
    "            picked_filemap = filemap_of_stage.sample(picks, with_replacement=False)\n",
    "            picked_images = picked_filemap.select(pl.col(\"raw\")).to_series().to_list()\n",
    "\n",
    "            return picked_images\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error in picking image within larval stage: {ls_beg}, {ls_end}. Error: {e}\"\n",
    "        )\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_images_from_filemap(\n",
    "    filemap_path,\n",
    "    database_config,\n",
    "    valid_scopes_expanded,\n",
    "    channel,\n",
    "    threshold=0.95,\n",
    "    lmbda=0.0075,\n",
    "    extra_adulthood_time=40,\n",
    "    n_picks=10,\n",
    "):\n",
    "    global variation_to_unified_scope_name\n",
    "    experiment_name = filemap_path.split(\"/\")[-4]\n",
    "    filemap_df = pl.read_csv(\n",
    "        filemap_path,\n",
    "        infer_schema_length=10000,\n",
    "        null_values=[\"np.nan\", \"[nan]\", \"\", \"NaN\", \"nan\", \"NA\", \"N/A\"],\n",
    "    )\n",
    "\n",
    "    # if Ignore is in the columns, remove all rows where Ignore is True\n",
    "    if \"Ignore\" in filemap_df.columns:\n",
    "        filemap_df = filemap_df.filter(~pl.col(\"Ignore\"))\n",
    "\n",
    "    # first check if the filemap has the required columns\n",
    "    worm_type_columns = [column for column in filemap_df.columns if \"qc\" in column]\n",
    "    channel_name = f'ch{channel[0] + 1}'\n",
    "    if \"ExperimentTime\" not in filemap_df.columns or filemap_df[\"ExperimentTime\"].is_null().all(): \n",
    "        # assume that ExperimentTime is in seconds and is equal to Time / 6 (10 min intervals)\n",
    "        # cast time to float\n",
    "        filemap_df = filemap_df.with_columns(\n",
    "            pl.col(\"Time\").cast(pl.Float64)\n",
    "        )\n",
    "        filemap_df = filemap_df.with_columns(\n",
    "            (pl.col(\"Time\")/6).alias(\"ExperimentTime\").cast(pl.Float64)\n",
    "        )\n",
    "    else:\n",
    "        # cast ExperimentTime to float\n",
    "        filemap_df = filemap_df.with_columns(\n",
    "            pl.col(\"ExperimentTime\").cast(pl.Float64)\n",
    "        )\n",
    "        filemap_df = filemap_df.with_columns(\n",
    "            (pl.col(\"ExperimentTime\")/3600).alias(\"ExperimentTime\")\n",
    "        )\n",
    "\n",
    "    database = pl.DataFrame()\n",
    "        \n",
    "    seg_columns = [column for column in filemap_df.columns if ((f'{channel_name}_seg' in column) and (\"str\" in column))]\n",
    "    straigthened_raw = f'{channel_name}_raw_str'\n",
    "    straigthened_raw_columns = [column for column in filemap_df.columns if (straigthened_raw in column)]\n",
    "    if len(straigthened_raw_columns) == 0:\n",
    "        print(\n",
    "            f\"Skipping experiment {experiment_name} as it does not have straightened raw images for channel {channel_name}.\"\n",
    "        )\n",
    "        return database\n",
    "    # remove the previous raw column if it exists\n",
    "    if \"raw\" in filemap_df.columns:\n",
    "        filemap_df = filemap_df.drop(\"raw\")\n",
    "    filemap_df = filemap_df.rename({straigthened_raw_columns[0]:\"raw\"})\n",
    "    volume_columns = [column for column in filemap_df.columns if ((channel_name in column) and (\"volume\" in column))]\n",
    "    # if len(worm_type_columns) == 0 or len(seg_columns) == 0 or len(volume_columns) == 0:\n",
    "    if len(worm_type_columns) == 0 or len(volume_columns) == 0:\n",
    "        print(\n",
    "            f\"Skipping experiment {experiment_name} as it does not have worm type or segmentation columns.\"\n",
    "        )\n",
    "        return database\n",
    "    seg_column = seg_columns[0]\n",
    "    worm_type_column = worm_type_columns[0]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    filemaps_of_points = filemap_df.partition_by(\"Point\")\n",
    "    stage_proportions = database_config.get(\"stage_proportions\", None)\n",
    "\n",
    "    microscope = [scope for scope in valid_scopes_expanded if scope in experiment_name][\n",
    "        0\n",
    "    ]\n",
    "    microscope = variation_to_unified_scope_name.get(microscope, microscope)\n",
    "\n",
    "\n",
    "    print(f\"Microscope found: {microscope} in experiment {experiment_name}\")\n",
    "\n",
    "\n",
    "    for filemap in filemaps_of_points:\n",
    "        # randomly ignore the point 50% of the time to reduce dataset size\n",
    "        if np.random.rand() < 0.25:\n",
    "            continue\n",
    "        if \"M4\" in filemap.columns:\n",
    "            # if M4 is annotated, remove rows where Time > M4 + extra_adulthood_time\n",
    "            m4 = filemap.select(pl.col(\"M4\")).row(0)[0]\n",
    "            if m4 is not None and not np.isnan(m4):\n",
    "                filemap = filemap.filter(\n",
    "                    pl.col(\"Time\") <= m4 + extra_adulthood_time\n",
    "                )\n",
    "\n",
    "        # remove rows with time too close to the molts\n",
    "        # we need to do this because the series is not smooth around the molts\n",
    "        molts = [\"M1\", \"M2\", \"M3\", \"M4\"]\n",
    "        for molt in molts:\n",
    "            if molt in filemap.columns:\n",
    "                molt_time = filemap.select(pl.col(molt)).row(0)[0]\n",
    "                if molt_time is not None and not np.isnan(molt_time):\n",
    "                    filemap = filemap.filter(\n",
    "                        (pl.col(\"Time\") < molt_time - 4)\n",
    "                        | (pl.col(\"Time\") > molt_time + 4)\n",
    "                    )\n",
    "\n",
    "        point = filemap.select(pl.col(\"Point\")).row(0)[0]\n",
    "        volume = filemap.select(pl.col(volume_columns[0])).to_numpy().squeeze()\n",
    "        # skip point if less than 10 data points\n",
    "        if np.sum(~np.isnan(volume)) < 10:\n",
    "            continue\n",
    "        worm_types = filemap.select(pl.col(worm_type_column)).to_numpy().squeeze()\n",
    "        corrected_volume = correct_series_with_classification(volume, worm_types)\n",
    "\n",
    "        experiment_time = filemap.select(pl.col(\"ExperimentTime\")).to_numpy().squeeze()\n",
    "\n",
    "        smoothed_volume = smooth_series_classified(\n",
    "            volume,\n",
    "            experiment_time,\n",
    "            qc=worm_types,\n",
    "            lmbda=lmbda,\n",
    "        )\n",
    "\n",
    "        normalized_residuals = (corrected_volume - smoothed_volume)/smoothed_volume\n",
    "\n",
    "        confirmed_errors = np.where(worm_types == 'error')[0]\n",
    "        threshold_value = np.quantile(np.abs(normalized_residuals), threshold)\n",
    "        potential_errors = np.where(np.abs(normalized_residuals) > threshold_value)[0]\n",
    "        potential_good = np.where(np.abs(normalized_residuals) <= threshold_value)[0]\n",
    "        # remove confirmed errors from potential errors\n",
    "        potential_errors = np.setdiff1d(potential_errors, confirmed_errors)\n",
    "        # remove confirmed errors from potential good\n",
    "        potential_good = np.setdiff1d(potential_good, confirmed_errors)\n",
    "    \n",
    "        # drop 95% of potential good to reduce dataset size\n",
    "        if len(potential_good) > 0:\n",
    "            potential_good = np.random.choice(\n",
    "                potential_good, size=int(len(potential_good) * 0.05), replace=False\n",
    "            )\n",
    "        eggs = np.where(worm_types == 'egg')[0]\n",
    "        \n",
    "\n",
    "            # Vectorized creation of all rows\n",
    "        indices = np.concatenate([potential_errors, potential_good, eggs, confirmed_errors])\n",
    "        classes = np.concatenate([\n",
    "            np.full(len(potential_errors), \"potential_error\"),\n",
    "            np.full(len(potential_good), \"good\"),\n",
    "            np.full(len(eggs), \"egg\"),\n",
    "            np.full(len(confirmed_errors), \"error\")\n",
    "        ])\n",
    "        \n",
    "        images = filemap.select(pl.col(\"raw\")).to_numpy().squeeze()[indices]\n",
    "        # replace external.data/TowbinLab by towbin.data/shared\n",
    "        images = [img.replace(\"TowbinLab\", \"shared\") for img in images]\n",
    "        images = [img.replace(\"external.data\", \"towbin.data\") for img in images]\n",
    "        \n",
    "        masks = filemap.select(pl.col(seg_column)).to_numpy().squeeze()[indices]\n",
    "        masks = [m.replace(\"TowbinLab\", \"shared\") for m in masks]\n",
    "        masks = [m.replace(\"external.data\", \"towbin.data\") for m in masks]\n",
    "\n",
    "        # only keep if both image and mask exist\n",
    "        valid_indices = [i for i, (img, msk) in enumerate(zip(images, masks)) if os.path.exists(img) and os.path.exists(msk)]\n",
    "        classes = [classes[i] for i in valid_indices]\n",
    "        images = [images[i] for i in valid_indices]\n",
    "        masks = [masks[i] for i in valid_indices]\n",
    "        filemap_rows = [\n",
    "            {\n",
    "                \"Class\": clss,\n",
    "                \"Image\": img,\n",
    "                \"Microscope\": microscope,\n",
    "                \"Point\": point,\n",
    "                \"Stage\": \"unknown\",\n",
    "                \"Mask\": mask\n",
    "            }\n",
    "            for clss, img, mask in zip(classes, images, masks)\n",
    "            # for clss, img in zip(classes, images)\n",
    "        ]\n",
    "        rows.extend(filemap_rows)\n",
    "    database = pl.DataFrame(\n",
    "        rows, \n",
    "        schema={\n",
    "            \"Class\": pl.Utf8,\n",
    "            \"Image\": pl.Utf8,\n",
    "            \"Microscope\": pl.Utf8,\n",
    "            \"Point\": pl.Int64,\n",
    "            \"Stage\": pl.Utf8,\n",
    "            \"Mask\": pl.Utf8,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return database\n",
    "\n",
    "\n",
    "def calculate_image_combinations(database_size, scope_proportions, stage_proportions):\n",
    "    \"\"\"Calculate number of images needed for each scope and stage combination.\"\"\"\n",
    "    combinations = {}\n",
    "\n",
    "    # Calculate for each scope and stage combination\n",
    "    for scope, scope_prop in scope_proportions.items():\n",
    "        combinations[scope] = {}\n",
    "        if not stage_proportions:\n",
    "            n_images = int(database_size * scope_prop)\n",
    "            combinations[scope][\"unknown\"] = n_images\n",
    "        else:\n",
    "            for stage, stage_prop in stage_proportions.items():\n",
    "                n_images = int(database_size * scope_prop * stage_prop)\n",
    "                combinations[scope][stage] = n_images\n",
    "\n",
    "    return combinations\n",
    "\n",
    "config_path = \"qc_dataset_config.yaml\"\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "storage_path = config.get(\"storage_path\", \"\")\n",
    "valid_subdirectories = config.get(\"valid_subdirectories\", [])\n",
    "\n",
    "database_path = config.get(\"database_path\", None)\n",
    "database_configs = config.get(\"database_configs\", {})\n",
    "extra_adulthood_time = config.get(\"extra_adulthood_time\", 0)\n",
    "valid_scopes = config.get(\"valid_scopes\", [])\n",
    "scopes_alt_names = config.get(\"scopes_alt_names\", {})\n",
    "keywords_to_exclude = config.get(\"keywords_to_exclude\", [])\n",
    "keywords_to_include = config.get(\"keywords_to_include\", [])\n",
    "experiments_to_consider = config.get(\"experiments_to_consider\", [])\n",
    "experiments_to_always_include = config.get(\"experiments_to_always_include\", [])\n",
    "experiments_to_exclude = config.get(\"experiments_to_exclude\", [])\n",
    "n_picks_per_stage = config.get(\"n_picks_per_stage\", 10)\n",
    "class_proportions = config.get(\"class_proportions\", {\"good\": 0.4, \"potential_error\": 0.25, \"error\": 0.25, \"egg\": 0.1})\n",
    "lmbda = config.get(\"lambda\", 0.0075)\n",
    "threshold = config.get(\"threshold\", 0.95)\n",
    "valid_scopes_expanded = []\n",
    "for scope in valid_scopes:\n",
    "    if scope in scopes_alt_names:\n",
    "        valid_scopes_expanded.extend(scopes_alt_names[scope])\n",
    "    else:\n",
    "        valid_scopes_expanded.append(scope)\n",
    "\n",
    "os.makedirs(database_path, exist_ok=True)\n",
    "\n",
    "# Create mapping from variations to standard names\n",
    "variation_to_unified_scope_name = {}\n",
    "for microscope, variations in scopes_alt_names.items():\n",
    "    for variation in variations:\n",
    "        variation_to_unified_scope_name[variation] = microscope\n",
    "\n",
    "os.makedirs(database_path, exist_ok=True)\n",
    "for sub_db in database_configs.keys():\n",
    "    sub_db_dir = os.path.join(database_path, sub_db)\n",
    "    os.makedirs(sub_db_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(sub_db_dir, \"images\"), exist_ok=True)\n",
    "    # os.makedirs(os.path.join(sub_db_dir, \"masks\"), exist_ok=True)\n",
    "\n",
    "# get all experiment directories\n",
    "experiment_directories = []\n",
    "valid_subdirectories = [\n",
    "    os.path.join(storage_path, sub_dir) for sub_dir in valid_subdirectories\n",
    "]\n",
    "if experiments_to_consider:\n",
    "    experiment_directories = experiments_to_consider\n",
    "else:\n",
    "    for exp_dir in valid_subdirectories:\n",
    "        experiment_directories.extend(\n",
    "            [\n",
    "                os.path.join(exp_dir, d)\n",
    "                for d in os.listdir(exp_dir)\n",
    "                if os.path.isdir(os.path.join(exp_dir, d))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "experiment_directories.extend(experiments_to_always_include)\n",
    "experiment_directories = list(set(experiment_directories))\n",
    "\n",
    "# filter experiment directories based on the criteria and get their filemaps\n",
    "for database_name, database_config in database_configs.items():\n",
    "    channel = database_config.get(\"channel\", [0])\n",
    "    get_annotated_only = database_config.get(\"stage_proportions\", None) is not None\n",
    "    filemaps = find_all_relevant_filemaps(\n",
    "        experiment_directories,\n",
    "        experiments_to_always_include,\n",
    "        keywords_to_include,\n",
    "        experiments_to_exclude,\n",
    "        valid_scopes_expanded,\n",
    "        keywords_to_exclude,\n",
    "        database_config,\n",
    "        get_annotated_only=get_annotated_only,\n",
    "    )\n",
    "\n",
    "    database = pl.DataFrame(schema={\n",
    "        \"Class\": pl.Utf8,\n",
    "        \"Image\": pl.Utf8,\n",
    "        \"Microscope\": pl.Utf8,\n",
    "        \"Point\": pl.Int64,\n",
    "        \"Stage\": pl.Utf8,\n",
    "        \"Mask\": pl.Utf8,\n",
    "    })\n",
    "    for filemap in filemaps:\n",
    "        try:\n",
    "            database = database.vstack(\n",
    "                get_images_from_filemap(\n",
    "                    filemap,\n",
    "                    database_config,\n",
    "                    valid_scopes_expanded,\n",
    "                    channel=channel,\n",
    "                    lmbda=lmbda,\n",
    "                    threshold=threshold,\n",
    "                    extra_adulthood_time=extra_adulthood_time,\n",
    "                    n_picks=n_picks_per_stage,\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing filemap {filemap}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # calculate number of images per class\n",
    "    class_counts = {}\n",
    "    total_images = database_config.get(\"size\", 1000)\n",
    "    for cls, prop in class_proportions.items():\n",
    "        class_counts[cls] = int(total_images * prop)\n",
    "    # sample images per class\n",
    "    sampled_database = pl.DataFrame(schema={\n",
    "        \"Class\": pl.Utf8,\n",
    "        \"Image\": pl.Utf8,\n",
    "        \"Microscope\": pl.Utf8,\n",
    "        \"Point\": pl.Int64,\n",
    "        \"Stage\": pl.Utf8,\n",
    "        \"Mask\": pl.Utf8,\n",
    "    })\n",
    "    for cls, count in class_counts.items():\n",
    "        class_images = database.filter(pl.col(\"Class\") == cls)\n",
    "        n = min(count, class_images.height)\n",
    "        class_images = class_images.sample(n, with_replacement=False)\n",
    "        sampled_database = sampled_database.vstack(class_images)\n",
    "\n",
    "    # sort the dataset by class to group similar images together\n",
    "    sampled_database = sampled_database.sort(\"Class\")\n",
    "    # create output names\n",
    "    def get_output_name(i, row):\n",
    "        return f'image_{i}_{row[\"Class\"]}_{row[\"Microscope\"]}.tiff'\n",
    "    sampled_database = sampled_database.with_columns(pl.arange(0, sampled_database.height).alias(\"Index\"))\n",
    "    sampled_database = sampled_database.with_columns(\n",
    "        pl.struct([\"Index\", \"Class\", \"Microscope\"])\n",
    "        .map_elements(lambda x: get_output_name(x[\"Index\"], x), return_dtype=pl.String)\n",
    "        .alias(\"OutputName\")\n",
    "    )\n",
    "    sampled_database = sampled_database.select(pl.exclude(\"Index\", \"Point\"))\n",
    "    sampled_database.write_csv(\n",
    "        os.path.join(\n",
    "            database_path, database_name, f\"{database_name}_classification_filemap.csv\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # copy the config file to the database directory\n",
    "    shutil.copy(\n",
    "        config_path,\n",
    "        os.path.join(database_path, database_name, f\"{database_name}_config.yaml\"),\n",
    "    )\n",
    "\n",
    "    images_output_dir = os.path.join(database_path, database_name, \"images\")\n",
    "    masks_output_dir = os.path.join(database_path, database_name, \"masks\")\n",
    "    os.makedirs(images_output_dir, exist_ok=True)\n",
    "    os.makedirs(masks_output_dir, exist_ok=True)\n",
    "    for row in tqdm(sampled_database.iter_rows(named=True), total=sampled_database.height):\n",
    "        image_path = row[\"Image\"]\n",
    "        mask_path = row[\"Mask\"]\n",
    "        image = read_tiff_file(image_path, channels_to_keep=channel)\n",
    "        mask = read_tiff_file(mask_path)\n",
    "        image_name = row[\"OutputName\"]\n",
    "        # save the image\n",
    "        imwrite(os.path.join(images_output_dir, image_name), image, compression=\"zlib\")\n",
    "        imwrite(os.path.join(masks_output_dir, image_name), mask, compression=\"zlib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "towbintools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
