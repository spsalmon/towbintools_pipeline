{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PCA Feature Analysis for Morphological Features\n",
    "Analyzes feature distributions, separability, and informativeness\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_features(X, y, feature_names=None, n_components=10):\n",
    "    \"\"\"\n",
    "    Comprehensive PCA analysis of features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target labels\n",
    "    feature_names : list, optional\n",
    "        Names of features\n",
    "    n_components : int\n",
    "        Number of PCA components to compute\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        feature_names = X.columns.tolist() if feature_names is None else feature_names\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FEATURE ANALYSIS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDataset shape: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    print(f\"Classes: {np.unique(y)} (counts: {np.bincount(y.astype(int))})\")\n",
    "    \n",
    "    # 1. Basic feature statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1. FEATURE STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    feature_stats = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean': X.mean(axis=0),\n",
    "        'std': X.std(axis=0),\n",
    "        'min': X.min(axis=0),\n",
    "        'max': X.max(axis=0),\n",
    "        'range': X.max(axis=0) - X.min(axis=0),\n",
    "        'zeros_%': (X == 0).mean(axis=0) * 100\n",
    "    })\n",
    "    \n",
    "    # Check for problematic features\n",
    "    low_variance = feature_stats[feature_stats['std'] < 1e-6]\n",
    "    if len(low_variance) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(low_variance)} features have near-zero variance!\")\n",
    "        print(low_variance[['feature', 'std']])\n",
    "    \n",
    "    constant_features = feature_stats[feature_stats['range'] < 1e-6]\n",
    "    if len(constant_features) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(constant_features)} features are constant!\")\n",
    "    \n",
    "    high_zeros = feature_stats[feature_stats['zeros_%'] > 90]\n",
    "    if len(high_zeros) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(high_zeros)} features are >90% zeros!\")\n",
    "        print(high_zeros[['feature', 'zeros_%']])\n",
    "    \n",
    "    # 2. Standardize features\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2. STANDARDIZING FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"‚úì Features standardized (mean=0, std=1)\")\n",
    "    \n",
    "    # 3. PCA Analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. PCA ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    n_components = min(n_components, X.shape[1], X.shape[0])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"\\nVariance explained by first {n_components} components:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "        cum_var = pca.explained_variance_ratio_[:i+1].sum()\n",
    "        print(f\"  PC{i+1}: {var*100:.2f}% (cumulative: {cum_var*100:.2f}%)\")\n",
    "    \n",
    "    cum_var_80 = np.where(np.cumsum(pca.explained_variance_ratio_) >= 0.80)[0]\n",
    "    if len(cum_var_80) > 0:\n",
    "        print(f\"\\nüí° {cum_var_80[0]+1} components explain 80% of variance\")\n",
    "    \n",
    "    cum_var_95 = np.where(np.cumsum(pca.explained_variance_ratio_) >= 0.95)[0]\n",
    "    if len(cum_var_95) > 0:\n",
    "        print(f\"üí° {cum_var_95[0]+1} components explain 95% of variance\")\n",
    "    \n",
    "    # 4. LDA Analysis (class separability)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. LINEAR DISCRIMINANT ANALYSIS (Class Separability)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_classes = len(np.unique(y))\n",
    "    lda_components = min(n_classes - 1, X.shape[1])\n",
    "    \n",
    "    if lda_components > 0:\n",
    "        try:\n",
    "            lda = LinearDiscriminantAnalysis(n_components=lda_components)\n",
    "            X_lda = lda.fit_transform(X_scaled, y)\n",
    "            \n",
    "            print(f\"\\nLDA variance explained (max {lda_components} discriminant axes):\")\n",
    "            for i, var in enumerate(lda.explained_variance_ratio_):\n",
    "                print(f\"  LD{i+1}: {var*100:.2f}%\")\n",
    "            \n",
    "            # Calculate separation between class means in LDA space\n",
    "            if lda_components >= 1:\n",
    "                class_means = np.array([X_lda[y == c].mean(axis=0) for c in np.unique(y)])\n",
    "                distances = []\n",
    "                for i in range(len(class_means)):\n",
    "                    for j in range(i+1, len(class_means)):\n",
    "                        dist = np.linalg.norm(class_means[i] - class_means[j])\n",
    "                        distances.append(dist)\n",
    "                print(f\"\\nüí° Average distance between class means in LDA space: {np.mean(distances):.3f}\")\n",
    "                print(f\"   (Higher is better for separability)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not perform LDA: {e}\")\n",
    "            X_lda = None\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Not enough classes for LDA\")\n",
    "        X_lda = None\n",
    "    \n",
    "    # 5. Feature correlation with target\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"5. FEATURE-TARGET RELATIONSHIPS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # For binary classification, compute point-biserial correlation\n",
    "    if len(np.unique(y)) == 2:\n",
    "        correlations = []\n",
    "        for i in range(X_scaled.shape[1]):\n",
    "            corr = np.corrcoef(X_scaled[:, i], y)[0, 1]\n",
    "            correlations.append(abs(corr))\n",
    "        \n",
    "        correlations = np.array(correlations)\n",
    "        top_indices = np.argsort(correlations)[-10:][::-1]\n",
    "        \n",
    "        print(\"\\nTop 10 features by correlation with target:\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"  {feature_names[idx]}: {correlations[idx]:.4f}\")\n",
    "        \n",
    "        if correlations.max() < 0.1:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: No features have correlation >0.1 with target!\")\n",
    "            print(\"   This suggests features may not be informative for this classification.\")\n",
    "    \n",
    "    # 6. Create visualizations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"6. GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot 1: Scree plot\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             pca.explained_variance_ratio_, 'bo-')\n",
    "    plt.xlabel('Principal Component', fontsize=12)\n",
    "    plt.ylabel('Variance Explained', fontsize=12)\n",
    "    plt.title('Scree Plot - Variance per Component', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Cumulative variance\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "             np.cumsum(pca.explained_variance_ratio_), 'ro-')\n",
    "    plt.axhline(y=0.8, color='g', linestyle='--', label='80% threshold')\n",
    "    plt.axhline(y=0.95, color='b', linestyle='--', label='95% threshold')\n",
    "    plt.xlabel('Number of Components', fontsize=12)\n",
    "    plt.ylabel('Cumulative Variance Explained', fontsize=12)\n",
    "    plt.title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: PCA scatter (PC1 vs PC2)\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', \n",
    "                         alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "    plt.title('PCA: First Two Components', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(scatter, label='Class')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Feature importance in PC1\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    pc1_importance = np.abs(pca.components_[0])\n",
    "    top_10_idx = np.argsort(pc1_importance)[-10:]\n",
    "    plt.barh(range(10), pc1_importance[top_10_idx])\n",
    "    plt.yticks(range(10), [feature_names[i] for i in top_10_idx], fontsize=9)\n",
    "    plt.xlabel('Absolute Loading', fontsize=12)\n",
    "    plt.title('Top 10 Features in PC1', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 5: LDA plot if available\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    if X_lda is not None and X_lda.shape[1] >= 2:\n",
    "        scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', \n",
    "                             alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "        plt.xlabel(f'LD1 ({lda.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "        if X_lda.shape[1] >= 2:\n",
    "            plt.ylabel(f'LD2 ({lda.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "        plt.title('LDA: Class Separability', fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(scatter, label='Class')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    elif X_lda is not None and X_lda.shape[1] == 1:\n",
    "        for class_val in np.unique(y):\n",
    "            plt.hist(X_lda[y == class_val, 0], alpha=0.5, label=f'Class {class_val}', bins=30)\n",
    "        plt.xlabel('LD1', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.title('LDA: 1D Projection', fontsize=14, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'LDA not available', ha='center', va='center', fontsize=14)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Plot 6: Feature variance distribution\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    feature_stds = X_scaled.std(axis=0)\n",
    "    try:\n",
    "        plt.hist(feature_stds, bins='auto', edgecolor='black')\n",
    "    except:\n",
    "        # If auto fails, just use bar plot\n",
    "        plt.bar(range(len(feature_stds)), sorted(feature_stds, reverse=True))\n",
    "        plt.xlabel('Feature Index (sorted)', fontsize=12)\n",
    "    plt.xlabel('Standard Deviation (after scaling)', fontsize=12)\n",
    "    plt.ylabel('Number of Features', fontsize=12)\n",
    "    plt.title('Distribution of Feature Variances', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/mnt/user-data/outputs/pca_feature_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"‚úì Saved: pca_feature_analysis.png\")\n",
    "    \n",
    "    # Additional plot: Pairplot of top PCA components\n",
    "    if n_components >= 3:\n",
    "        fig2, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                ax = axes[i, j]\n",
    "                if i == j:\n",
    "                    # Histogram on diagonal\n",
    "                    for class_val in np.unique(y):\n",
    "                        ax.hist(X_pca[y == class_val, i], alpha=0.5, \n",
    "                               label=f'Class {class_val}', bins=30)\n",
    "                    ax.set_ylabel('Frequency')\n",
    "                    if i == 0:\n",
    "                        ax.legend()\n",
    "                else:\n",
    "                    # Scatter plot\n",
    "                    scatter = ax.scatter(X_pca[:, j], X_pca[:, i], c=y, \n",
    "                                       cmap='viridis', alpha=0.5, s=10)\n",
    "                \n",
    "                if i == 2:\n",
    "                    ax.set_xlabel(f'PC{j+1} ({pca.explained_variance_ratio_[j]*100:.1f}%)')\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(f'PC{i+1} ({pca.explained_variance_ratio_[i]*100:.1f}%)')\n",
    "                \n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('PCA Components Pairplot', fontsize=16, fontweight='bold', y=1.00)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/mnt/user-data/outputs/pca_pairplot.png', dpi=150, bbox_inches='tight')\n",
    "        print(\"‚úì Saved: pca_pairplot.png\")\n",
    "    \n",
    "    # 7. Summary and recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"7. SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check for issues\n",
    "    issues = []\n",
    "    if pca.explained_variance_ratio_[0] > 0.9:\n",
    "        issues.append(\"‚ö†Ô∏è  First PC explains >90% variance - features may be highly redundant\")\n",
    "    \n",
    "    if len(low_variance) > 0:\n",
    "        issues.append(f\"‚ö†Ô∏è  {len(low_variance)} features have near-zero variance\")\n",
    "    \n",
    "    if len(np.unique(y)) == 2:\n",
    "        if correlations.max() < 0.1:\n",
    "            issues.append(\"‚ö†Ô∏è  No strong feature-target correlations (<0.1)\")\n",
    "        elif correlations.max() < 0.2:\n",
    "            issues.append(\"‚ö†Ô∏è  Weak feature-target correlations (<0.2)\")\n",
    "    \n",
    "    cum_var_3 = np.sum(pca.explained_variance_ratio_[:3])\n",
    "    if cum_var_3 < 0.5:\n",
    "        issues.append(f\"‚ö†Ô∏è  First 3 PCs only explain {cum_var_3*100:.1f}% variance - high dimensionality\")\n",
    "    \n",
    "    if len(issues) > 0:\n",
    "        print(\"\\nüî¥ Issues Detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No major issues detected!\")\n",
    "    \n",
    "    print(\"\\nüìã Recommendations:\")\n",
    "    print(\"  1. Check the PCA scatter plot - do classes visually separate?\")\n",
    "    print(\"  2. Check the LDA plot - better separation = more informative features\")\n",
    "    print(\"  3. If separation is poor, consider:\")\n",
    "    print(\"     - Engineering new features (ratios, differences, interactions)\")\n",
    "    print(\"     - Checking if features are computed correctly from images\")\n",
    "    print(\"     - Trying different morphological features\")\n",
    "    print(\"     - Examining misclassified samples visually\")\n",
    "    \n",
    "    return {\n",
    "        'pca': pca,\n",
    "        'lda': lda if X_lda is not None else None,\n",
    "        'X_pca': X_pca,\n",
    "        'X_lda': X_lda,\n",
    "        'scaler': scaler,\n",
    "        'feature_stats': feature_stats\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: How to use this script\")\n",
    "print(\"=\"*80)\n",
    "# Load your data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/mnt/towbin.data/shared/spsalmon/towbinlab_classification_database/datasets/10x_pharynx_qc/pharynx/features.csv\")\n",
    "target = pd.read_csv(\"/mnt/towbin.data/shared/spsalmon/towbinlab_classification_database/datasets/10x_pharynx_qc/pharynx/processed_annotations.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)  # or specify feature columns\n",
    "y = df['target']\n",
    "\n",
    "# Run analysis\n",
    "from pca_feature_analysis import analyze_features\n",
    "results = analyze_features(X, y, feature_names=X.columns, n_components=10)\n",
    "\n",
    "# Access results\n",
    "# results['pca'] - fitted PCA object\n",
    "# results['X_pca'] - transformed data\n",
    "# results['feature_stats'] - feature statistics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "towbintools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
